# Credit Card Default
## Dataset: 
The dataset used in this project is based on credit card payment information of 30,000 customers from Taiwan, collected over six months from April 2005 to September 2005. This dataset has been widely used in previous studies and has been considered an important benchmark dataset for credit risk analysis. The dataset contains 23 attributes including demographic information such as age, sex, and marital status, as well as credit card-specific information such as payment history, amount of bill statement, and payment amount. The main goal of the previous studies that used this dataset was to predict the likelihood of default on credit card payments using different machine learning and statistical techniques. The literature suggests that this dataset has provided promising results in developing accurate credit risk models, and the features included in the dataset have been found to be important predictors of credit risk.

## Procedure:
The dataset was preprocessed by changing certain feature names, synthetically creating more data using SMOTE, and handling multiple data within a feature that used different labels but meant the same thing (different labels for others and unknowns).
We then performed feature engineering, selecting relevant features and standardizing certain features of our data. The standardized features included age, credit limit, payment status, payment history, bill amount, and payment amount. We performed dimensionality reduction using PCA and LDA.
We used cross-validation with five folds to evaluate different models' performance. The validation set was used to select the best-performing model, tune the hyperparameters, and prevent overfitting. We used the F1-score, accuracy, and confusion matrix as evaluation metrics.
We then compared the performance of various models, including Baseline (nearest means classifier), Trivial system (a system that outputs class assignments at random), logistic regression, XGBOOST, SVM, PCA, and LDA. We used the scikit-learn library for implementing most of these models, while NumPy implemented the trivial system and baseline method.
The best-performing model was selected based on the average F1 score obtained from cross-validation. The test set was used to evaluate the selected model's generalization performance. We used the test set only once after the model selection process to prevent overfitting.
In summary, the train-test split was already provided, and we performed feature engineering and dimensionality reduction, used cross-validation with five folds, selected the best-performing model based on the average F1-score, and evaluated the selected model's generalization performance using the test set. 
